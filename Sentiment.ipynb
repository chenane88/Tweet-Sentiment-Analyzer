{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jDezN2o5C9y",
        "outputId": "1a1fc99c-63b2-4750-cd82-53d56320027e"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-3AhxJu5WVS",
        "outputId": "7f7ff0a7-5998-486d-eb74-6c3677997131"
      },
      "source": [
        "nltk.download('twitter_samples')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMcHahlI5fNp"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzReTTfYYbOg"
      },
      "source": [
        "Using NLTK for natural language processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QH8WypeZbPA"
      },
      "source": [
        "Text processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HabqRgkT4oUC"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "positive_tweets =twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets =twitter_samples.strings('negative_tweets.json')\n",
        "example_postive_tweet=positive_tweets[0]\n",
        "example_negative_tweet=negative_tweets[0]\n",
        "test_pos = positive_tweets[4000:]\n",
        "train_pos = positive_tweets[:4000]\n",
        "test_neg = negative_tweets[4000:]\n",
        "train_neg = negative_tweets[:4000]\n",
        "train_x = train_pos + train_neg \n",
        "test_x = test_pos + test_neg\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM3OlsHHY40S"
      },
      "source": [
        "tokenization and stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8gIAWik4_jl"
      },
      "source": [
        "import re                                  \n",
        "import string\n",
        "from nltk.corpus import stopwords          \n",
        "from nltk.stem import PorterStemmer        \n",
        "from nltk.tokenize import TweetTokenizer\n",
        "def text_process(tweet):\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    tokenizer = TweetTokenizer()\n",
        "    tweet_tokenized = tokenizer.tokenize(tweet)\n",
        "    stopwords_english = stopwords.words('english') \n",
        "    tweet_processsed=[word for word in tweet_tokenized \n",
        "    if word not  in stopwords_english and word not in       \n",
        "    string.punctuation]\n",
        "    stemmer = PorterStemmer() \n",
        "    tweet_after_stem=[]\n",
        "    for word in tweet_processsed:\n",
        "        word=stemmer.stem(word)\n",
        "        tweet_after_stem.append(word)\n",
        "    return tweet_after_stem"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0joXEQcv7TAY",
        "outputId": "300c256a-f1c1-4d09-c212-913d23f8fee0"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPiQ3BU0YkYX"
      },
      "source": [
        "features extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpT8pqwZ5nog"
      },
      "source": [
        "pos_words=[]\n",
        "for tweet in positive_tweets:\n",
        "    tweet=text_process(tweet)\n",
        "    \n",
        "    for word in tweet:\n",
        "        \n",
        "        pos_words.append(word)\n",
        "freq_pos={}\n",
        "for word in pos_words:\n",
        "    if (word,1) not in freq_pos:\n",
        "        freq_pos[(word,1)]=1\n",
        "    else:\n",
        "        freq_pos[(word,1)]=freq_pos[(word,1)]+1\n",
        "neg_words=[]\n",
        "for tweet in negative_tweets:\n",
        "    tweet=text_process(tweet)\n",
        "    \n",
        "    for word in tweet:\n",
        "        \n",
        "        neg_words.append(word)\n",
        "freq_neg={}\n",
        "for word in neg_words:\n",
        "    if (word,0) not in freq_neg:\n",
        "        freq_neg[(word,0)]=1\n",
        "    else:\n",
        "        freq_neg[(word,0)]=freq_neg[(word,0)]+1\n",
        "freqs_dict = dict(freq_pos)\n",
        "freqs_dict.update(freq_neg)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZqddYVX5yDd"
      },
      "source": [
        "import numpy as np\n",
        "def features_extraction(tweet, freqs_dict):\n",
        "    word_l = text_process(tweet)\n",
        "    x = np.zeros((1, 3))\n",
        "    x[0,0] = 1 \n",
        "    for word in word_l:\n",
        "        try:\n",
        "            x[0,1] += freqs_dict[(word,1)]\n",
        "        except:\n",
        "            x[0,1] += 0\n",
        "        try: \n",
        "            x[0,2] += freqs_dict[(word,0.0)]\n",
        "        except:\n",
        "            x[0,2] += 0\n",
        "    assert(x.shape == (1, 3))\n",
        "    return x\n",
        "X = np.zeros((len(train_x), 3))\n",
        "    \n",
        "for i in range(len(train_x)):\n",
        "    \n",
        "    X[i, :]= features_extraction(train_x[i], freqs_dict)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUBXD8hCZsxj"
      },
      "source": [
        "creating the sentiment classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvX2i3EI7dwZ"
      },
      "source": [
        "def sigmoid(x): \n",
        "    h = 1/(1+np.exp(-x))\n",
        "    return h\n",
        "\n",
        "def gradientDescent_algo(x, y, theta, alpha, num_iters):\n",
        "    m = x.shape[0]\n",
        "    for i in range(0, num_iters):\n",
        "        z = np.dot(x,theta)\n",
        "        h = sigmoid(z)\n",
        "        J = -1/m*(np.dot(y.T,np.log(h))+np.dot((1-y).T,np.log(1-h)))\n",
        "        theta = theta-(alpha/m)*np.dot(x.T,h-y)\n",
        "    J = float(J)\n",
        "    return J, theta"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RefU-Dd1Z1UP"
      },
      "source": [
        "Training and Evaluating the sentiment classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DupVWBbh8Gb2"
      },
      "source": [
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :]= features_extraction(train_x[i], freqs_dict)\n",
        "Y = train_y\n",
        "J, theta = gradientDescent_algo(X, Y, np.zeros((3, 1)), 1e-9, 1500)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLiL50eo8I-I"
      },
      "source": [
        "def predict(tweet, freqs_dict, theta):\n",
        "    x = features_extraction(tweet,freqs_dict)\n",
        "    y_pred = sigmoid(np.dot(x,theta))\n",
        "    return y_pred\n",
        "def test_accuracy(test_x, test_y, freqs_dict, theta):\n",
        "    y_hat = []\n",
        "    for tweet in test_x:\n",
        "        \n",
        "        y_pred = predict(tweet, freqs_dict, theta)\n",
        "        \n",
        "        if y_pred > 0.5:\n",
        "           \n",
        "            y_hat.append(1)\n",
        "        else:\n",
        "            \n",
        "            y_hat.append(0)\n",
        "    m=len(y_hat)\n",
        "    y_hat=np.array(y_hat)\n",
        "    y_hat=y_hat.reshape(m)\n",
        "    test_y=test_y.reshape(m)\n",
        "    \n",
        "    c=y_hat==test_y\n",
        "    j=0\n",
        "    for i in c:\n",
        "        if i==True:\n",
        "            j=j+1\n",
        "    accuracy = j/m\n",
        "    return accuracy\n",
        "accuracy = test_accuracy(test_x, test_y, freqs_dict, theta)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGTccL8C8Pmj",
        "outputId": "51fd8c71-3d1c-438f-e82e-ce907eb9fea4"
      },
      "source": [
        "print(accuracy)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2pgfatBZ9a5"
      },
      "source": [
        "we get 98 percent accuracy meaning the model is almost perfect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4woLzTVw8Xte"
      },
      "source": [
        "my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COhwYZ1CGHWR",
        "outputId": "3842b45f-3f72-481b-c46e-1c5595debe62"
      },
      "source": [
        "print(features_extraction(my_tweet,1))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPTP1RwAZER5"
      },
      "source": [
        "testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41Zm1UaTHHBZ",
        "outputId": "54fbba34-cffe-41f7-da6d-9bd09030dac2"
      },
      "source": [
        "y_hat = predict(my_tweet, freqs_dict, theta)\n",
        "print(y_hat)\n",
        "if y_hat > 0.5:\n",
        "    print('Positive sentiment')\n",
        "else: \n",
        "    print('Negative sentiment')\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.40496694]]\n",
            "Negative sentiment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG1g5-OWIBPw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}